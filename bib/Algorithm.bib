
@misc{weak_2024,
	title = {Weak error analysis for strong approximation schemes of {SDEs} with super-linear coefficients {II}: finite moments and higher-order schemes},
	shorttitle = {Weak error analysis for strong approximation schemes of {SDEs} with super-linear coefficients {II}},
	url = {http://arxiv.org/abs/2406.14065},
	abstract = {This paper is the second in a series of works on weak convergence of one-step schemes for solving stochastic differential equations (SDEs) with one-sided Lipschitz conditions. It is known that the super-linear coefficients may lead to a blowup of moments of solutions and numerical solutions and thus affect the convergence of numerical methods. Wang et al. (2023, IMA J. Numer. Anal.) have analyzed weak convergence of one-step numerical schemes when solutions to SDEs have all finite moments. Therein some modified Euler schemes have been discussed about their weak convergence orders. In this work, we explore the effects of limited orders of moments on the weak convergence of a family of explicit schemes. The schemes are based on approximations/modifications of terms in the Ito-Talyor expansion. We provide a systematic but simple way to establish weak convergence orders for these schemes. We present several numerical examples of these schemes and show their weak convergence orders.},
	urldate = {2024-10-30},
	publisher = {arXiv},
	author = {Zhao, Yuying and Wang, Xiaojie and Zhang, Zhongqiang},
	month = oct,
	year = {2024},
	note = {arXiv:2406.14065},
	keywords = {Computer Science - Numerical Analysis, Mathematics - Numerical Analysis, Mathematics - Probability},
	file = {Preprint PDF:files/8/Zhao …等 - 2024 - Weak error analysis for strong approximation schemes of SDEs with super-linear coefficients II fini.pdf:application/pdf;Snapshot:files/9/2406.html:text/html},
}

@misc{restart_2023,
	title = {Restart {Sampling} for {Improving} {Generative} {Processes}},
	url = {http://arxiv.org/abs/2306.14878},
	abstract = {Generative processes that involve solving differential equations, such as diffusion models, frequently necessitate balancing speed and quality. ODE-based samplers are fast but plateau in performance while SDE-based samplers deliver higher sample quality at the cost of increased sampling time. We attribute this difference to sampling errors: ODE-samplers involve smaller discretization errors while stochasticity in SDE contracts accumulated errors. Based on these findings, we propose a novel sampling algorithm called Restart in order to better balance discretization errors and contraction. The sampling method alternates between adding substantial noise in additional forward steps and strictly following a backward ODE. Empirically, Restart sampler surpasses previous SDE and ODE samplers in both speed and accuracy. Restart not only outperforms the previous best SDE results, but also accelerates the sampling speed by 10-fold / 2-fold on CIFAR-10 / ImageNet \$64 {\textbackslash}times 64\$. In addition, it attains significantly better sample quality than ODE samplers within comparable sampling times. Moreover, Restart better balances text-image alignment/visual quality versus diversity than previous samplers in the large-scale text-to-image Stable Diffusion model pre-trained on LAION \$512 {\textbackslash}times 512\$. Code is available at https://github.com/Newbeeer/diffusion\_restart\_sampling},
	urldate = {2024-10-30},
	publisher = {arXiv},
	author = {Xu, Yilun and Deng, Mingyang and Cheng, Xiang and Tian, Yonglong and Liu, Ziming and Jaakkola, Tommi},
	month = nov,
	year = {2023},
	note = {arXiv:2306.14878},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Computation, Statistics - Machine Learning},
	file = {Preprint PDF:files/17/Xu - 2023 - Restart Sampling for Improving Generative Processes.pdf:application/pdf;Snapshot:files/18/2306.html:text/html},
}

@misc{noauthor_unige_nodate,
	title = {{UNIGE} 14x050 - {Deep} {Learning}},
	url = {https://fleuret.org/dlc/},
	urldate = {2024-10-30},
	file = {UNIGE 14x050 - Deep Learning:files/20/dlc.html:text/html},
}

@misc{song_generative_2020,
	title = {Generative {Modeling} by {Estimating} {Gradients} of the {Data} {Distribution}},
	url = {http://arxiv.org/abs/1907.05600},
	abstract = {We introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. Because gradients can be ill-defined and hard to estimate when the data resides on low-dimensional manifolds, we perturb the data with different levels of Gaussian noise, and jointly estimate the corresponding scores, i.e., the vector fields of gradients of the perturbed data distribution for all noise levels. For sampling, we propose an annealed Langevin dynamics where we use gradients corresponding to gradually decreasing noise levels as the sampling process gets closer to the data manifold. Our framework allows flexible model architectures, requires no sampling during training or the use of adversarial methods, and provides a learning objective that can be used for principled model comparisons. Our models produce samples comparable to GANs on MNIST, CelebA and CIFAR-10 datasets, achieving a new state-of-the-art inception score of 8.87 on CIFAR-10. Additionally, we demonstrate that our models learn effective representations via image inpainting experiments.},
	urldate = {2024-10-31},
	publisher = {arXiv},
	author = {Song, Yang and Ermon, Stefano},
	month = oct,
	year = {2020},
	note = {arXiv:1907.05600},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:files/22/Song和Ermon - 2020 - Generative Modeling by Estimating Gradients of the Data Distribution.pdf:application/pdf;Snapshot:files/23/1907.html:text/html},
}

@article{master_thesis,
	title = {Master {Thesis} : {Diffusion} models : seek of information and structure in latent space},
	language = {en},
	author = {Maziane, Y},
	file = {PDF:files/24/Maziane - Master Thesis  Diffusion models  seek of information and structure in latent space.pdf:application/pdf},
}

@misc{luo_understanding_2022,
	title = {Understanding {Diffusion} {Models}: {A} {Unified} {Perspective}},
	shorttitle = {Understanding {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2208.11970},
	abstract = {Diffusion models have shown incredible capabilities as generative models; indeed, they power the current state-of-the-art models on text-conditioned image generation such as Imagen and DALL-E 2. In this work we review, demystify, and unify the understanding of diffusion models across both variational and score-based perspectives. We first derive Variational Diffusion Models (VDM) as a special case of a Markovian Hierarchical Variational Autoencoder, where three key assumptions enable tractable computation and scalable optimization of the ELBO. We then prove that optimizing a VDM boils down to learning a neural network to predict one of three potential objectives: the original source input from any arbitrary noisification of it, the original source noise from any arbitrarily noisified input, or the score function of a noisified input at any arbitrary noise level. We then dive deeper into what it means to learn the score function, and connect the variational perspective of a diffusion model explicitly with the Score-based Generative Modeling perspective through Tweedie's Formula. Lastly, we cover how to learn a conditional distribution using diffusion models via guidance.},
	language = {en},
	urldate = {2024-11-11},
	publisher = {arXiv},
	author = {Luo, Calvin},
	month = aug,
	year = {2022},
	note = {arXiv:2208.11970 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {PDF:files/28/Luo - 2022 - Understanding Diffusion Models A Unified Perspective.pdf:application/pdf},
}

@misc{mother_2021,
	title = {Score-{Based} {Generative} {Modeling} through {Stochastic} {Differential} {Equations}},
	url = {http://arxiv.org/abs/2011.13456},
	abstract = {Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field ({\textbackslash}aka, score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.},
	urldate = {2024-11-11},
	publisher = {arXiv},
	author = {Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P. and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
	month = feb,
	year = {2021},
	note = {arXiv:2011.13456},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:files/50/Song - 2021 - Score-Based Generative Modeling through Stochastic Differential Equations.pdf:application/pdf;Snapshot:files/51/2011.html:text/html},
}

@misc{lu_maximum_2022,
	title = {Maximum {Likelihood} {Training} for {Score}-{Based} {Diffusion} {ODEs} by {High}-{Order} {Denoising} {Score} {Matching}},
	url = {http://arxiv.org/abs/2206.08265},
	abstract = {Score-based generative models have excellent performance in terms of generation quality and likelihood. They model the data distribution by matching a parameterized score network with first-order data score functions. The score network can be used to define an ODE ("score-based diffusion ODE") for exact likelihood evaluation. However, the relationship between the likelihood of the ODE and the score matching objective is unclear. In this work, we prove that matching the first-order score is not sufficient to maximize the likelihood of the ODE, by showing a gap between the maximum likelihood and score matching objectives. To fill up this gap, we show that the negative likelihood of the ODE can be bounded by controlling the first, second, and third-order score matching errors; and we further present a novel high-order denoising score matching method to enable maximum likelihood training of score-based diffusion ODEs. Our algorithm guarantees that the higher-order matching error is bounded by the training error and the lower-order errors. We empirically observe that by high-order score matching, score-based diffusion ODEs achieve better likelihood on both synthetic data and CIFAR-10, while retaining the high generation quality.},
	urldate = {2024-11-11},
	publisher = {arXiv},
	author = {Lu, Cheng and Zheng, Kaiwen and Bao, Fan and Chen, Jianfei and Li, Chongxuan and Zhu, Jun},
	month = jun,
	year = {2022},
	note = {arXiv:2206.08265},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:files/53/Lu - 2022 - Maximum Likelihood Training for Score-Based Diffusion ODEs by High-Order Denoising Score Matching.pdf:application/pdf;Snapshot:files/54/2206.html:text/html},
}

@misc{noauthor_sde_nodate,
	title = {diffusion_model{Scientific} {Spaces}},
	url = {https://spaces.ac.cn/archives/9209/comment-page-1},
	urldate = {2024-11-11},
	file = {Scientific Spaces:files/56/comment-page-1.html:text/html},
}

@inproceedings{DDPM_2020,
 author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {6840--6851},
 publisher = {Curran Associates, Inc.},
 title = {Denoising Diffusion Probabilistic Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf},
 volume = {33},
 year = {2020}
}

