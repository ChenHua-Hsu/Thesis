
@misc{vaswani_attention_2023,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2024-10-30},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = aug,
	year = {2023},
	note = {arXiv:1706.03762},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Attention is All You Need},
	file = {Preprint PDF:files/11/Vaswani …等 - 2023 - Attention Is All You Need.pdf:application/pdf;Snapshot:files/12/1706.html:text/html},
}

@misc{kach_pay_2024,
	title = {Pay {Attention} {To} {Mean} {Fields} {For} {Point} {Cloud} {Generation}},
	url = {http://arxiv.org/abs/2408.04997},
	abstract = {Collider data generation with machine learning has become increasingly popular in particle physics due to the high computational cost of conventional Monte Carlo simulations, particularly for future high-luminosity colliders. We propose a generative model for point clouds that employs an attention-based aggregation while preserving a linear computational complexity with respect to the number of points. The model is trained in an adversarial setup, ensuring input permutation equivariance and invariance for the generator and critic, respectively. To stabilize known unstable adversarial training, a feature matching loss is introduced. We evaluate the performance on two different datasets. The former is the top-quark JETNET150 dataset, where the model outperforms the current state-of-the-art GAN-based model, despite having significantly fewer parameters. The latter is dataset 2 of the CaloChallenge, which comprises point clouds with up to 30× more points compared to the first dataset. The model and its corresponding code are available at https://github.com/kaechb/MDMA/tree/NeurIPS.},
	language = {en},
	urldate = {2024-11-11},
	publisher = {arXiv},
	author = {Käch, Benno and Melzer-Pellmann, Isabell and Krücker, Dirk},
	month = aug,
	year = {2024},
	note = {arXiv:2408.04997 [hep-ex]},
	keywords = {High Energy Physics - Experiment},
	file = {PDF:files/30/Käch …等 - 2024 - Pay Attention To Mean Fields For Point Cloud Generation.pdf:application/pdf},
}

@misc{tancik_fourier_2020,
	title = {Fourier {Features} {Let} {Networks} {Learn} {High} {Frequency} {Functions} in {Low} {Dimensional} {Domains}},
	url = {http://arxiv.org/abs/2006.10739},
	abstract = {We show that passing input points through a simple Fourier feature mapping enables a multilayer perceptron (MLP) to learn high-frequency functions in lowdimensional problem domains. These results shed light on recent advances in computer vision and graphics that achieve state-of-the-art results by using MLPs to represent complex 3D objects and scenes. Using tools from the neural tangent kernel (NTK) literature, we show that a standard MLP fails to learn high frequencies both in theory and in practice. To overcome this spectral bias, we use a Fourier feature mapping to transform the effective NTK into a stationary kernel with a tunable bandwidth. We suggest an approach for selecting problem-speciﬁc Fourier features that greatly improves the performance of MLPs for low-dimensional regression tasks relevant to the computer vision and graphics communities.},
	language = {en},
	urldate = {2024-11-11},
	publisher = {arXiv},
	author = {Tancik, Matthew and Srinivasan, Pratul P. and Mildenhall, Ben and Fridovich-Keil, Sara and Raghavan, Nithin and Singhal, Utkarsh and Ramamoorthi, Ravi and Barron, Jonathan T. and Ng, Ren},
	month = jun,
	year = {2020},
	note = {arXiv:2006.10739 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Project page: https://people.eecs.berkeley.edu/{\textasciitilde}bmild/fourfeat/},
	file = {PDF:files/32/Tancik …等 - 2020 - Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains.pdf:application/pdf},
}
