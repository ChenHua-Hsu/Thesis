\chapter{Strategies and Results}
\section{Data Preprocessing}

\subsection{Bucketing}
Before we explain why we need bucketing, we can first explain the structure of our data. When one particle interacts with the detector, it will produce a series of hits, which we call one shower. So in one shower, we have several hits, while one hit means one point in the detector labeled by the energy. One hit has several features, such as the hit energy, x, y and z coordinates. What's more, we will send several showers make it to be one batch to our model. So the structure of our data is actually a 3D tensor, where the first dimension is the number of showers, the second dimension is the number of hits in one shower, and the third dimension is the number of features in one hit.

In chapter 5, we have discussed that our model is a transformer-based model. While transformer implement the self-attention mechanism, it requires the length of the sequence to be fixed in each batches. However, the number of hits in each event varies, which makes it difficult to feed the data into the transformer. To address this issue, we would need to pad the sequences to a fixed length. What's more, the length of the data can vary from 1 to 5500, which means that the padding will be very large. This will lead to a waste of memory and computation. To solve this problem, we employed a bucketing strategy to group events with similar numbers of hits into the same bucket. This allowed us to pad the sequences within each bucket to a fixed length, making it easier to feed the data into the transformer. Based on the principle of similar memory usage, we divided the data into 45 buckets, each containing events with a similar number of hits. This bucketing strategy significantly improved the efficiency of the model and reduced the computational burden. Another advantage of bucketing is that we can first train the model on a smaller bucket to see if the model can learn the data well. If the model can learn the data well, we can then train the model on a larger bucket. This allows us to gradually increase the complexity of the data and ensure that the model can handle the data effectively.
\subsection{Preprocessor}

Preprocessing is a crucial step in preparing data for machine learning. Raw data often contains missing values, outliers, and features on different scales, which can negatively impact model performance. Effective preprocessing cleans and standardizes the data, ensuring consistency and enabling accurate predictions. It also helps models learn specific relationships between features more effectively.

A key role of preprocessing is improving data quality. Techniques like imputation, normalization, and outlier removal address missing or noisy values, allowing models to focus on meaningful patterns rather than irrelevant or erroneous information. Preprocessing also standardizes feature scales, ensuring equal contributions to models, which is especially critical for distance-based algorithms like neural networks or support vector machines.

Additionally, preprocessing optimizes computational efficiency by simplifying data complexity through methods like dimensionality reduction or sampling. This is vital for large-scale datasets, enabling faster and more resource-efficient training while preserving essential information. Overall, preprocessing is foundational for reliable, robust machine learning systems.

One important point to note is that we chose to use the x,y coordinate system instead of the cylindrical coordinate system. The primary reason for this choice is the discontinuity at $\theta=0$ and $\theta=2\pi$, which is unphysical and can introduce challenges during training. Although the cylindrical coordinate system aligns better with the detector structure and may simplify learning the relationship between radius and energy, we opted for the x,y coordinate system to ensure continuity and avoid such complications. 

From the reasons above, we employ three different data preprocessing techniques for detector hit information: \textbf{RobustScaler}, \textbf{QuantileTransformer}, and \textbf{Exponential Transformation}. While the the comparison between three methods focus on transforming the x and y coordinates, the energy and z coordinate are processed using the same methodology across all three approaches. This consistent treatment of energy and z coordinates allows for a direct comparison of the methods and highlights the benefits of the different transformations applied to x and y.

\noindent
\textbf{Energy Transformation}

The energy transformation applies a \textbf{logit-based rescaling}, ensuring numerical stability and normalization. Given the raw hit energy \( e \), the transformation is defined as:

\begin{equation}
e' = \log \left( \frac{1 + (1 - 2\times10^{-6}) \frac{e}{E_{\text{incident}} }}{1 - \frac{e}{E_{\text{incident}} }} \right)
\end{equation}

where \( E_{\text{incident}} \) is the incident particle energy. This formulation ensures stability while preserving the ratio of the deposited energy to the incident energy. The advantages of this approach include:

\begin{itemize}
    \item \textbf{Prevents numerical instability}: The small offset ensures that divisions by zero do not occur.
    \item \textbf{Incident Related Logit Transformation}: The transformation densifies the distribution of energy values and reduces variance between different incident energy levels.
\end{itemize}

\noindent
\textbf{\( z \)-Coordinate Transformation}  

The \( z \)-coordinate transformation applies a linear rescaling:

\begin{equation}
z' = \frac{z - z_{\min}}{z_{\max} - z_{\min}}
\end{equation}

ensuring values remain within a fixed range while preserving spatial relationships. Benefits include:

\begin{itemize}
    \item \textbf{Normalization improves model stability}: A fixed range enhances model generalization.
    \item \textbf{Outlier Cut Easier}: Knowing detector boundaries (\( z_{\max}, z_{\min} \)), we can discard predictions outside \( (0,1) \).
\end{itemize}

Above are the preprocessing methods for energy and \( z \). Next, we introduce preprocessing methods for \( x \) and \( y \).


\noindent
\begin{itemize}
    \item \textbf{RobustScaler on x and y} 
    
    The \texttt{RobustScaler} removes the median and scales data using the interquartile range (IQR), making it \textbf{robust to outliers}. The transformation is:

    \[
    x' = \frac{x - \text{Median}}{\text{IQR}}
    \]
    
    where \( \text{IQR} = Q3 - Q1 \) represents the range between the 75th and 25th percentiles. This transformation is particularly effective in datasets with extreme values. Advantages include:

    \begin{itemize}
        \item \textbf{Resistant to Outliers}: Using the median and IQR minimizes the influence of extreme values.
        \item \textbf{Preservation of Relative Distances}: The transformation retains the original distribution while normalizing the scale.
        \item \textbf{Effective for Skewed Data}: Works well on data with heavy tails or asymmetric distributions.
    \end{itemize}

    \begin{figure}[h!]
        \centering
        \includegraphics[width=0.6\textwidth]{Figures/robustscalar.png} 
        \caption{RobustScaler}\label{fig:robustscalar}
    \end{figure}

    \item \textbf{QuantileTransformer on x and y}

    As for \texttt{QuantileTransformer}, it is a non-linear transformation that maps data to a uniform or normal distribution. Here I choose the normal one.  It applies a non-linear transformation using the empirical cumulative distribution function (ECDF) to reshape the feature's distribution. This ensures that each feature closely resembles the desired target distribution.

    This method is particularly useful when the data distribution has heavy tails or abrupt changes, as our x and y coordinates do. By transforming the data to a normal distribution, the \texttt{QuantileTransformer} can help the model learn the underlying patterns more effectively. This is especially beneficial for our data, as it can improve the model's ability to capture the relationship between energy and radius. The advantages of the \texttt{QuantileTransformer} include:


    \begin{itemize}
        \item \textbf{Uniform-to-Normal Mapping}: Converts arbitrary distributions into a normal distribution, aiding model interpretability.
        \item \textbf{Outlier Robustness}: Reduces the influence of extreme values using empirical percentiles.
        \item \textbf{Smooth Data Representation}: Reshapes skewed or heavy-tailed distributions into a well-behaved normal form.
    \end{itemize}

    \begin{figure}[h!]
        \centering
        \includegraphics[width=0.6\textwidth]{Figures/quantileTransfomrer.png} 
        \caption{QuantileTransformer}\label{fig:quantileTransfomrer}
    \end{figure}

    \item \textbf{Exponential Transformation}

    The \textbf{Exponential Transformation} follows a \textbf{sigmoid-based scaling}:

    \begin{equation}
    x', y' = \frac{1}{1 + e^{-0.07 \cdot (x, y)}}
    \end{equation}

    which maps original \( x, y \) coordinates into a compressed range, preventing extreme values from dominating. Advantages include:

    \begin{itemize}
        \item \textbf{Soft bounding of values}: Ensures large deviations do not dominate the scale.
        \item \textbf{Improved gradient stability}: The sigmoid function provides smooth gradients, improving model training.
        \item \textbf{Consistent mapping}: Unlike statistics-based transformations, this method applies a continuous function, making it robust for out-of-distribution inputs.
    \end{itemize}

    \begin{figure}[h!]
        \centering
        \includegraphics[width=0.6\textwidth]{Figures/original.png}
        \caption{Exponential Transformation}\label{fig:exponential_transformation}
    \end{figure}
\end{itemize}

The figures \ref{fig:robustscalar}, \ref{fig:quantileTransfomrer}, and \ref{fig:exponential_transformation} show the distribution of data after applying different preprocessing methods. More figures can be found in Appendix \ref{AppendixA}. 

From these results, we see that the \texttt{QuantileTransformer} performs best for \( x \) and \( y \) because it transforms data into a normal distribution, allowing the model to better capture spatial patterns and the relationship between energy and radius.


\section{Metrics}
\subsection{FID Score}
To evaluate the performance of our model, we employed the Fréchet Inception Distance (FID) score as a key metric. The FID score is widely used to assess the quality of generated samples by measuring the distance between the feature representations of real and generated images using the InceptionV3 model \cite{inceptionv3}. A lower FID score indicates that the generated samples are closer to the real samples in terms of their statistical distribution. We utilized the PyTorch library's implementation of the FID score \cite{pytorch} for our calculations. The FID score is calculated as follows:

For two multivariate Gaussian distributions with means $\mu_{\text{real}}$ and $\mu_{\text{gen}}$ and covariance matrices $\Sigma_{\text{real}}$ and $\Sigma_{\text{gen}}$, the FID score is given by:

\begin{equation}
    \text{FID} = ||\mu_{\text{real}} - \mu_{\text{gen}}||^2 + \text{Tr}(\Sigma_{\text{real}} + \Sigma_{\text{gen}} - 2(\Sigma_{\text{real}}\Sigma_{\text{gen}})^{1/2}),
\end{equation}

In order to measure what's the performance on each dimension, we also calculate the FID score on each dimension. Then the FID score on each dimension is calculated as follows:

\begin{equation}
    \text{FID}_{\text{dim}} = ||\mu_{\text{real}} - \mu_{\text{gen}}||^2 + \text{Tr}(\sigma_{\text{real}} + \sigma_{\text{gen}} - 2(\sigma_{\text{real}}\sigma_{\text{gen}})^{1/2}),
\end{equation}

One important point to note is that sometimes the FID score is not enough to evaluate the performance of the model. For example, if the FID score is low, it means that the generated samples are close to the real samples in terms of their statistical distribution. However, the generated samples may not capture the underlying physics of the data, for example, the shape of the data may not be the gaussian distribution. In this case, the FID score may not be a good metric to evaluate the performance of the model. So when we evaluate the performance of the model, we still need to rely on other metrics and observation.

\subsection{Classifier}

As mentioned earlier, the FID score alone is insufficient for evaluating the performance of the model. To complement it, we employ classifiers to assess the model's ability to generate realistic samples. These classifiers are binary, designed to distinguish between real and generated samples. The structure of the classifiers is primarily based on deep neural networks (DNNs). The input features for the classifiers can range from high-level features, such as energy distributions across layers or $\theta$ bins, to low-level features like the energy values in each voxel. Regardless of the input, real samples are labeled as 1, and generated samples are labeled as 0. The loss function used is the Binary Cross-Entropy Loss (\texttt{BCEWithLogitsLoss}).

However, our classifiers consistently achieve very high performance, with an AUC of 99--100\%. This indicates that it is relatively easy for the classifier to distinguish between real and generated samples. This issue is not unique to our study; many papers report similar findings, even when their models achieve low FID scores and realistic data shapes. One plausible explanation is that generated samples tend to exhibit higher continuity, while real data has inherent discreteness due to the limitations of the detector. This mismatch in continuity could make it easier for classifiers to identify generated samples.

% Add this section's details as necessary.

\section{VE and VP Studies}
As mentioned before, there are two main ways to add the noise into the data, which are Variance Exploding (VE) and Variance Preserving (VP). In this section, we will discuss the performance of the model trained with these two methods. First, we can observe the standard deviation times the noise we add in, which is the change of every step. We can see that it is more steep and the value is bigger for VE method. This means that the VE method has more power to push the data to the random noise, which is the initial state of the sampling space. That is why we guess the VE method will have a better performance than the VP method.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth,height= 0.35\textwidth]{Figures/sde1.png}
    % Include your loss shape figure here
    \caption{Comparison of VE and VP methods for both $\sigma_{max} = 1, \sigma_{min} = 0.0001$} \label{fig:sde1}
\end{figure}

From Figure \ref{fig:sde1}, it may not be obvious that the value of VE is larger, but later if we see Figure \ref{fig:sde5} and \ref{fig:sde10} when $\sigma_{max} = 5, \sigma_{max} = 10$, we can see that the value of VE is larger than VP.(0.3 vs 0.075) This is consistent with our guess that the VE method will have a better performance than the VP method.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth,height = 0.35\textwidth]{Figures/sde5.png}
    % Include your loss shape figure here
    \caption{Comparison of VE and VP methods for both $\sigma_{max} = 5, \sigma_{min} = 0.0001$}\label{fig:sde5}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth, height = 0.35\textwidth]{Figures/sde10.png}
    % Include your loss shape figure here
    \caption{Comparison of VE and VP methods for both $\sigma_{max} = 10, \sigma_{min} = 0.0001$}\label{fig:sde10}
\end{figure}

Next, we can further compare the actual distribution change after adding the noise. We can see that the distribution of the data after adding the noise in the VE method is more close to the random noise than the VP method. This is consistent with our guess that the VE method will have a better performance than the VP method.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/VE_5_0.0001.png}
    % Include your loss shape figure here
    \caption{The distribution of the data after adding the noise using VE method.}\label{fig:ve}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/VP_5_0.0001.png}
    % Include your loss shape figure here
    \caption{The distribution of the data after adding the noise using VP method.} \label{fig:vp}
\end{figure}

From Figure \ref{fig:ve} and \ref{fig:vp}, we can observe two points. Firstly, the VE method requires more steps to effectively disrupt the original data distribution, allowing the reverse process to provide the model with additional opportunities to capture the true distribution, which is advantageous. Secondly, the distribution of data subjected to noise through the VE method appears closer to random noise than that of the VP method. This observation supports our hypothesis that the VE method is likely to outperform the VP method. 

We also compared the FID scores of models trained with the VE and VP methods. The results showed that the VE method resulted in a lower FID score compared to the VP method. This suggests that the VE method is more effective at pushing the model toward generating random samples that better represent the initial sampling space.

% \begin{table}[h!]
%     \centering
%     % Include your FID score comparison figure here
%     \caption{Comparison of FID scores for VE and VP methods.}
% \end{table}

\begin{table}[h]
    \centering
    \renewcommand{\arraystretch}{1.3} % Adjust row height for better readability
    \setlength{\tabcolsep}{8pt} % Adjust column spacing
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        \textbf{FID}  & \textbf{10} & \textbf{5} & \textbf{1} & \textbf{0.5} \\
        \hline
        VE           & 0.0312440  & 0.0349802  & 0.0224611  & 0.0190552  \\
        VP           & 0.0058973 & 10.533640   & 104.77893 & 0.0107441  \\
        \hline
    \end{tabular}
    
    \vspace{0.6em}
    
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        \textbf{FID\_e}  & \textbf{10} & \textbf{5} & \textbf{1} & \textbf{0.5} \\
        \hline
        VE               & 0.0334921  & 0.0059905  & 0.0158524  & 0.0195042  \\
        VP               & 0.0356144  & 8.2458230   & 84.672020   & 0.0225893  \\
        \hline
    \end{tabular}
    
    \vspace{0.6em}
    
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        \textbf{FID\_x}  & \textbf{10} & \textbf{5} & \textbf{1} & \textbf{0.5} \\
        \hline
        VE               & 0.0001294  & 0.0001524  & 0.0001418  & 0.0001736  \\
        VP               & 0.0001456  & 2.1163020    & 114.35691   & 0.0001689  \\
        \hline
    \end{tabular}
    
    \vspace{0.6em}
    
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        \textbf{FID\_z}  & \textbf{10} & \textbf{5} & \textbf{1} & \textbf{0.5} \\
        \hline
        VE               & 0.0007689  & 0.0006219  & 0.0007465  & 0.0012151   \\
        VP               & 0.0015260  & 1.7053030    & 27.805210    & 0.0011892   \\
        \hline
    \end{tabular}

    \caption{FID, FID\_e, FID\_x, and FID\_z values for different SDE, $\sigma_{max}$, and $\sigma_{min}$.}
    \label{tab:fid_values}
\end{table}



In conclusion, the VE method outperformed the VP method in terms of FID score. 
We guess this is because it has more power to push our data to random noise, which is the initial state of sampling space. So our model know how to do the reverse process at the beginning in VE method. For example, if we see the standard deviation of both VE and VP methods, one can find out VE has the steeper slope than VP, which means it has the power to push the data to the random noise.

\section{$\sigma_{max}$ and $\sigma_{min}$ Studies}

Among all fo the parameters, the $\sigma_{max}$ may be the most important one. In the context of diffusion models, the parameters $\sigma_{max}$ and $\sigma_{min}$ play a crucial role in determining the noise levels introduced during the forward and backward processes. These parameters define the range of noise scales, influencing both the quality of the generated samples and the training stability of the model. This section explores the impact of $\sigma_{max}$ and $\sigma_{min}$ on model performance and provides insights into selecting optimal values for these parameters.

\subsection{The Role of $\sigma_{max}$ and $\sigma_{min}$}

The parameter $\sigma_{min}$ represents the minimum noise level in the forward process and also used as the step size of $\sigma$ series. In this case as you can imagine, $\sigma_{min}$ is typically set close to zero. However, based on our abservation, $\sigma_{min}$ won't actually affect too mcuh on the performance of our model. Conversely, $\sigma_{max}$ deos. It defines the maximum noise level and is set high enough to approximate a standard normal distribution. And it also determines the power to change our data distribution during the training. These noise levels influence the progression of the diffusion process, as the model learns to reverse the added noise during training.

Larger $\sigma_{max}$ ensures sufficient diversity in the data during the forward process, helping the model generalize better. Yet, if $\sigma_{max}$ is too large, it can result in excessively noisy samples, making it challenging for the model to learn the reverse process effectively.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.4\textwidth} % Slightly increase width to reduce gaps
        \includegraphics[width=\textwidth]{Figures/vp_20.png}
        \caption{VP with $\sigma_{max} = 20$}
        \label{fig:vp_20}
    \end{subfigure}
    \hspace{0.015\textwidth} % Reduce horizontal space
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{Figures/vp_10.png}
        \caption{VP with $\sigma_{max} = 10$}
        \label{fig:vp_10}
    \end{subfigure}
    
    \vspace{0.4em} % Reduce vertical space between rows
    
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{Figures/vp_5.png}
        \caption{VP with $\sigma_{max} = 5$}
        \label{fig:vp_5}
    \end{subfigure}
    \hspace{0.015\textwidth} % Reduce horizontal space
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{Figures/vp_1.png}
        \caption{VP with $\sigma_{max} = 1$}
        \label{fig:vp_1}
    \end{subfigure}
    
    \caption{The result of different $\sigma_{max}$ in VP.}
    \label{fig:vp_sigma_max}
\end{figure}


\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.4\textwidth} % Adjusted width to 0.4
        \includegraphics[width=\textwidth]{Figures/ve_20.png}
        \caption{VE with $\sigma_{max} = 20$}
        \label{fig:ve_20}
    \end{subfigure}
    \hspace{0.015\textwidth} % Reduce horizontal space
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{Figures/ve_10.png}
        \caption{VE with $\sigma_{max} = 10$}
        \label{fig:ve_10}
    \end{subfigure}
    
    \vspace{0.4em} % Reduce vertical space between rows
    
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{Figures/ve_5.png}
        \caption{VE with $\sigma_{max} = 5$}
        \label{fig:ve_5}
    \end{subfigure}
    \hspace{0.015\textwidth} % Reduce horizontal space
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{Figures/ve_1.png}
        \caption{VE with $\sigma_{max} = 1$}
        \label{fig:ve_1}
    \end{subfigure}
    
    \caption{The result of different $\sigma_{max}$ in VE.}
    \label{fig:ve_sigma_max}
\end{figure}


And we can further check the sweep in Figure \ref{fig:sde_sweep} and \ref{fig:sde_sweep_sigmamax}. One can see that the performance of the model is better when $\sigma_{max}$ is larger. This actually fit with our prediction as the reasons above. 

\begin{figure}[h!]
    \centering
    % Include your figure here
    \includegraphics[width=0.8\textwidth]{Figures/sde_sweep.png}
    \caption{The result of different $\sigma_{max}$ and $\sigma_{min}$ in VE.} \label{fig:sde_sweep}
\end{figure}

\begin{figure}
    \centering
    % Include your figure here
    \includegraphics[width=0.8\textwidth]{Figures/sde_sweep_sigmamax.png} 
    \caption{The result of fig \ref{fig:sde_sweep}, but grouped by $\sigma_{max}$ in VE.}\label{fig:sde_sweep_sigmamax}
\end{figure}

\subsection{Conclusions}

As shown in the results, the choice of $\sigma_{max}$ and $\sigma_{min}$ significantly impacts the performance of the model. Larger $\sigma_{max}$ values can improve the diversity of the data and enhance the model's generalization ability. However, setting $\sigma_{max}$ too high can lead to noisy samples and hinder the model's learning process. On the other hand, $\sigma_{min}$ has a less pronounced effect on model performance, as it primarily serves as the step size for the noise levels. And based on our data scale, we choose $\sigma_{min}$ to be 0.0003, and $\sigma_{max}$ to be 5.0 in VE.

\section{Overall Parameter Sweeping}
Besides, the parameters mentioned above there are also a lot of other parameters that can affect the performance of the model or the memory allocation. Thus, we conducted a parameter sweeping study using \texttt{wandb}. We experimented with various learning rates, batch sizes, and hidden dimensions. Our findings indicated that the best-performing parameter configuration was:
\begin{itemize}
    \item Learning rate: $0.0003$
    \item Batch size: $128$
    \item Embedding dimension: $96$
    \item Hidden dimension: $96$
    \item Number of Attention Heads: $8$
    \item Number of Encoder Blocks : $16$
    \item Dropout rate: $0.2$
    \item Sampler Step: $100$
    \item Correction Step: $25$
    \item SDE : VE
    \item Sigma Max: $5.0$
    \item Sigma Min: $0.0003$
\end{itemize}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/other_sweep.png}
    % Include your parameter sweeping study figure here
    \caption{Visualization of parameter sweeping results.}
\end{figure}

% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=0.8\textwidth]{Figures/sde_importance.png}
%     % Include your figure here
%     \caption{The result of energy deposit of single bucket data and all bucket data.}
% \end{figure}


\section{Centralization}
After visualizing 2D or 3D plots revealed that the model failed to capture the relationship between energy and radius. A key observation was that the model could not learn that higher energy values should be concentrated near the center (smaller radii). Consequently, while the 1D plots were satisfactory, the generated samples lacked proper centralization.

To address this, we first tried to transform our data into spherical coordinate and introduce a correlation term between energy and theta in the loss function to try to suppress relation between energy and theta, hoping our model can thus learn more about the relation between energy and radius. 

The new loss function is defined as:
\begin{equation}
    L = L_{\text{MSE}} + \lambda L_{\text{cor}},
\end{equation}
where $L_{\text{MSE}}$ is the mean squared error loss, $L_{\text{cor}}$ is the correlation loss, and $\lambda$ is a weighting factor for the correlation loss. The correlation loss is defined as:
\begin{equation}
    L_{\text{cor}} = \frac{1}{N} \sum_{i=1}^{N} (x_i - \bar{x})(y_i - \bar{y}),
\end{equation}
where $x$ and $y$ are the variables of interest, and $\bar{x}$ and $\bar{y}$ are their respective means.

The reason why we don't apply the correlation term between energy and radius is that the relation between them is by experienced, although everyone would expect the result, it's not solid, we don't want to bias our model, or you can say we don't want to tell the answer of the relation to our model. 

However, although the correlation term was added to the loss function and it indeed suppressed the relation between energy and theta, the centralization of the generated samples did not improve significantly. This suggests that the correlation term alone is not sufficient to address the centralization issue.

% \begin{figure}[h!]
%     \centering
%     % Include any relevant results or visualizations here
%     \caption{The Picture after adding the correlation term.}
% \end{figure}

After that, one time when we tried to use QuantileTransformer to preprocess the data, we found that the centralization of the data is improved. This is because the QuantileTransformer can transform the data to follow a uniform or a normal distribution. This can help the model to learn the data better, especailly the x,y distribution. This also makes it is able to learn the relation between energy and radius better.

The result compared to the original one is shown in Figure \ref{fig:centralization}.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.5\textwidth} % Adjust width as needed
        \includegraphics[width=\textwidth]{Figures/comparison_original.png}
        \caption{The energy in each voxel with original transformation.}
        \label{fig:original}
    \end{subfigure}
    \hfill % Optional: Adds space between the two subfigures
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{Figures/comparison.png}
        \caption{The energy in each voxel with QuantileTransformer transformation.}
        \label{fig:quantile}
    \end{subfigure}
    
    \caption{The Comparison Picture after using QuantileTransformer.}
    \label{fig:centralization}
\end{figure}

\section{Conditioning Issue}
\subsection{Incident energy}
With the optimal settings, our model was able to generate the basic shapes of both the energy and spatial distributions. However, the model often produced an excessive number of hits (\texttt{nhits}) at higher energy levels, leading to overestimation. This issue was not observed when training on single-bucket data, indicating that the model struggles to differentiate between data from different buckets. This suggests that our conditional variables are not functioning effectively. As you can see the result of energy deposit of single bucket data and all bucket data, the model can generate the data well in single bucket data, but it failed to generate the data well in all bucket data. This is because the model can't learn the condition well.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.45\textwidth} % Adjust width as needed
        \includegraphics[width=\textwidth]{Figures/energy_voxel_single.png}
        \caption{The energy deposit of single bucket data.}
        \label{fig:single_bucket}
    \end{subfigure}
    \hfill % Optional: Adds space between the two subfigures
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{Figures/energy_voxel_full.png}
        \caption{The energy deposit of five bucket data.}
        \label{fig:all_bucket}
    \end{subfigure}
    
    \caption{he result of energy deposit of single bucket data and all bucket data.}
    \label{fig:energy_voxel}
\end{figure}

To address this issue, we first need to make sure if our conditional variables aren't really working. So we tried to add the incident energy as the conditional variables and not. The result is shown below:

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.45\textwidth} % Adjust width as needed
        \includegraphics[width=\textwidth]{Figures/energy_deposit_without_incident.png}
        \caption{The energy deposit without incident energy.}
        \label{fig:without_incident}
    \end{subfigure}
    \hfill % Optional: Adds space between the two subfigures
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{Figures/energy_voxel_full.png}
        \caption{The energy deposit with incident energy.}
        \label{fig:with_incident}
    \end{subfigure}
    
    \caption{Main caption for both figures}
    \label{fig:main}
\end{figure}

They are basically the same, indicating that the conditional variables are not working. Next, we also tried to concatenate the incident energy with the input data instead of adding them and the result is shown Figure \ref{fig:concatenate}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/concatenate.png}
    % Include your figure here
    \caption{The result of energy deposit with incident energy concatenated with the input data.} \label{fig:concatenate}
\end{figure}

As you can see, the result is totally a disaster. To be honest, we still don't know why this happened. And that is also one of the things we need to figure out in the future. We will probably try to use fewer hits and focus on one or two dimensions to find out the reason.

% Next, we move to even more complex conditional variables. We have tried class token. T



% The result is shown in Figure ~\ref{fig:energy_deposit}:

% \begin{figure}[ht]
%     \centering
%     % Include your figure here
%     \caption{The result of energy deposit with class token.}\label{fig:energy_deposit}
% \end{figure}

\subsection{Time}

We also attempted to incorporate time as a conditional variable in our model. However, the results differ from those observed with incident energy. Even without explicitly using time as a condition, the output layer implicitly incorporates its effect by dividing by the standard deviation of the stochastic differential equation (SDE), which is time-dependent. Despite this, the inclusion of time as a conditional variable does not appear to enhance the model's performance.

The plot of loss versus time reveals consistent behavior across epochs, showing that the shape of this plot remains virtually unchanged. Notably, the loss value at $t = 0$ is almost identical to the initial loss, indicating that the time condition fails to improve the model's capacity to learn the data effectively.

Time close to $t = 0$ represents the critical phase where the model transitions toward generating real data, whereas near $t = 1$, the model predominantly learns the structure of Gaussian noise. This suggests that the model's learning mechanism may inherently prioritize earlier time steps, making additional time conditioning redundant or ineffective.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.45\textwidth} % Adjust width as needed
        \includegraphics[width=\textwidth]{Figures/loss_t_0.png}
        \caption{}
        \label{fig:loss_t_0}
    \end{subfigure}
    \hfill % Optional: Adds space between the two subfigures
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{Figures/loss_t_10.png}
        \caption{}
        \label{fig:loss_t_10}
    \end{subfigure}
    
    \caption{The left figure shows the loss at epoch 0, which is quite normal it's still caotic. The right figure actually represent the loss after 10 epochs.}
    \label{fig:loss_t}
\end{figure}

From the Figure ~\ref{fig:loss_t}, We can see that the loss value near t equals to 0 is always aorund 1, which is as same as it is at the initial state. This means that our model learns nothing in that region. And the region stands for our model to predict the real data. That's may be the reason why our model can't learn the real shape of the data well. It only learns the approximate shape of the data from time is above 0.4.

\section{Conclusion}

In this work, we explored various data preprocessing techniques and model configurations to improve the performance of our transformer-based generative model for detector hit data. We implemented multiple strategies to optimize the data representation and ensure efficient learning.

First, we introduced a \textbf{bucketing strategy} to handle the variability in sequence lengths, significantly reducing memory overhead while improving computational efficiency. This allowed us to maintain a structured approach to feeding data into the transformer, ensuring stable training dynamics.

For \textbf{preprocessing}, we applied three distinct transformations—\textbf{RobustScaler, QuantileTransformer, and Exponential Transformation}—to normalize the hit coordinate data while ensuring robustness to outliers. We observed that QuantileTransformer provided the best performance, as it effectively reshaped the data into a normal distribution, improving the model's ability to capture spatial relationships and energy dependencies.

Through extensive experimentation with variance exploding (VE) and variance preserving (VP) stochastic differential equations, we found that VE outperforms VP in terms of pushing the data distribution towards an effective generative space, resulting in lower FID scores and better model convergence.

We also examined the effect of key hyperparameters, particularly $\sigma_{max}$ and $\sigma_{min}$, in controlling the diffusion process. Our results indicate that a larger $\sigma_{max}$ improves the diversity and realism of generated samples by facilitating a more expressive transformation of the data, while $\sigma_{min}$ had a minor impact on overall performance.

To assess model quality, we used the Fréchet Inception Distance (FID) score, supplemented with a classifier-based evaluation. While the classifier achieved near-perfect performance in distinguishing real and generated samples, we observed that real-world constraints and detector properties introduce inherent discreteness that can be challenging for the generative model to replicate.

One significant challenge we encountered was the conditioning issue, particularly with incident energy and time as conditional variables. Despite various conditioning strategies, including direct concatenation and implicit conditioning through normalization, the model struggled to fully leverage these inputs. This suggests that additional work is required to refine the conditional mechanisms to improve control over generated samples.

Additionally, we observed a **centralization issue** in the generated data, where the model failed to accurately capture the expected energy-radius relationship. Our attempts to enforce correlation constraints showed limited improvement, but the **QuantileTransformer preprocessing unexpectedly enhanced centralization**, highlighting its potential importance in data representation.

Moving forward, future work will focus on:
\begin{itemize}
    \item Improving the conditioning mechanism to ensure that incident energy and other physical parameters effectively guide the generation process.
    \item Investigating alternative loss functions and regularization techniques to better capture the physical constraints of the detector.
    \item Exploring architectural modifications, such as hybrid transformer-CNN approaches, to better leverage spatial dependencies in hit distributions.
    \item Refining the preprocessing pipeline by testing other transformations that could further enhance the model's ability to generalize across different hit distributions.
\end{itemize}

In conclusion, while our model demonstrates strong generative capabilities and promising results, further refinement is needed to fully capture the underlying physics of detector hit data. The findings in this work provide a solid foundation for future improvements in data-driven generative modeling in high-energy physics applications.




