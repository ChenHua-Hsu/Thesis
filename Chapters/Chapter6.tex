\chapter{Strategies and Results}
\section{Data Preprocessing}

\subsection{Bucketing}
Before we explain why we need bucketing, we can first explain the structure of our data. When one particle interacts with the detector, it will produce a series of hits, which we call one shower. So in one shower, we have several hits, while one hit means one point in the detector labeled by the energy. One hit has several features, such as the hit energy, x, y and z coordinates. What's more, we will send several showers make it to be one batches to our model. So the structure of our data is actually a 3D tensor, where the first dimension is the number of showers, the second dimension is the number of hits in one shower, and the third dimension is the number of features in one hit.

In chapter 5, we have discussed that our model is a transformer-based model. While transformer implement the self-attention mechanism, it requires the length of the sequence to be fixed in each batches. However, the number of hits in each event varies, which makes it difficult to feed the data into the transformer. To address this issue, we would need to pad the sequences to a fixed length. What's more, the length of the data can vary from 1 to 5500, which means that the padding will be very large. This will lead to a waste of memory and computation. To solve this problem, we employed a bucketing strategy to group events with similar numbers of hits into the same bucket. This allowed us to pad the sequences within each bucket to a fixed length, making it easier to feed the data into the transformer. Based on the principle of similar memory usage, we divided the data into 45 buckets, each containing events with a similar number of hits. This bucketing strategy significantly improved the efficiency of the model and reduced the computational burden. Another advantage of bucketing is that we can first train the model on a smaller bucket to see if the model can learn the data well. If the model can learn the data well, we can then train the model on a larger bucket. This allows us to gradually increase the complexity of the data and ensure that the model can handle the data effectively.
\subsection{Preprocessor}

Preprocessing is a crucial step in preparing data for machine learning. Raw data often contains missing values, outliers, and features on different scales, which can negatively impact model performance. Effective preprocessing cleans and standardizes the data, ensuring consistency and enabling accurate predictions. It also helps models learn specific relationships between features more effectively.

A key role of preprocessing is improving data quality. Techniques like imputation, normalization, and outlier removal address missing or noisy values, allowing models to focus on meaningful patterns rather than irrelevant or erroneous information. Preprocessing also standardizes feature scales, ensuring equal contributions to models, which is especially critical for distance-based algorithms like neural networks or support vector machines.

Additionally, preprocessing optimizes computational efficiency by simplifying data complexity through methods like dimensionality reduction or sampling. This is vital for large-scale datasets, enabling faster and more resource-efficient training while preserving essential information. Overall, preprocessing is foundational for reliable, robust machine learning systems.

From the reasons above, studying which preprocessing method is the best for our data is necessary. Below are some preprocessing methods we have tried and their results.

\begin{itemize}
    \item \textbf{RobustScalar}
    \begin{figure}[ht]
        \centering
        % Include your figure here
        \caption{RobustScalar}
    \end{figure}
    \item \textbf{QuantileTransformer}
    \begin{figure}[ht]
        \centering
        % Include your figure here
        \caption{RobustScalar}
    \end{figure}
    \item \textbf{StandardScaler}
    \begin{figure}[ht]
        \centering
        % Include your figure here
        \caption{RobustScalar}
    \end{figure}
    \item \textbf{MinMaxScaler}
    \begin{figure}[ht]
        \centering
        % Include your figure here
        \caption{RobustScalar}
    \end{figure}
    \item \textbf{Specail Transformation from other papers}
    \begin{figure}[ht]
        \centering
        % Include your figure here
        \caption{RobustScalar}
    \end{figure}
\end{itemize}


The reason of choosing x y coordinate rather than spherical coordinate
\section{Metrics}
\subsection{FID Score}
To evaluate the performance of our model, we employed the Fr√©chet Inception Distance (FID) score as a key metric. The FID score is widely used to assess the quality of generated samples by measuring the distance between the feature representations of real and generated images using the InceptionV3 model \cite{inceptionv3}. A lower FID score indicates that the generated samples are closer to the real samples in terms of their statistical distribution. We utilized the PyTorch library's implementation of the FID score \cite{pytorch} for our calculations.

\subsection{Classifier}
% Add this section's details as necessary.

\section{VE and VP Studies}
To begin, we compared the performance of the Variance Exploding (VE) and Variance Preserving (VP) methods. Both methods were used to train the model, and their results were evaluated. First, we observed that the loss curves during training exhibit distinct shapes for the two methods.

\begin{figure}[h!]
    \centering
    % Include your loss shape figure here
    \caption{Comparison of training loss curves for VE and VP methods.}
\end{figure}

We also compared the FID scores of models trained with the VE and VP methods. The results showed that the VE method resulted in a lower FID score compared to the VP method. This suggests that the VE method is more effective at pushing the model toward generating random samples that better represent the initial sampling space.

\begin{figure}[h!]
    \centering
    % Include your FID score comparison figure here
    \caption{Comparison of FID scores for VE and VP methods.}
\end{figure}

In conclusion, the VE method outperformed the VP method in terms of FID score. 
We guess this is because it has more power to push our data to random noise, which is the initial state of sampling space. So our model know how to do the reverse process at the beginning in VE method. For example, if we see the standard deviation of both VE and VP methods, one can find out VE has the steeper slope than VP, which means it has the power to push the data to the random noise.

\begin{figure}[h!]
    \centering
    % Include your FID score comparison figure here
    \caption{Comparison of FID scores for VE and VP methods.}
\end{figure}

\section{Parameter Sweeping}
To optimize the model, we conducted a parameter sweeping study using \texttt{wandb}. We experimented with various learning rates, batch sizes, and hidden dimensions. Our findings indicated that the best-performing parameter configuration was:
\begin{itemize}
    \item Learning rate: $0.0003$
    \item Batch size: $128$
    \item Embedding dimension: $96$
    \item Hidden dimension: $96$
\end{itemize}

\begin{figure}[h!]
    \centering
    % Include your parameter sweeping study figure here
    \caption{Visualization of parameter sweeping results.}
\end{figure}

\section{Energy Distribution}
With the optimal settings, our model was able to generate the basic shapes of both the energy and spatial distributions. However, the model often produced an excessive number of hits (\texttt{nhits}) at higher energy levels, leading to overestimation. This issue was not observed when training on single-bucket data, indicating that the model struggles to differentiate between data from different buckets. This suggests that our conditional variables are not functioning effectively.

\section{Centralization}
In addition to the conditional variable issue, visualizing 2D or 3D plots revealed that the model failed to capture the relationship between energy and radius. A key observation was that the model could not learn that higher energy values should be concentrated near the center (smaller radii). Consequently, while the 1D plots were satisfactory, the generated samples lacked proper centralization.

To address this, we first tried to transform our data into spherical coordinate and introduce a correlation term between energy and theta in the loss function to try to suppress relation between energy and theta, hoping our model can thus learn more about the relation between energy and radius. 

The new loss function is defined as:
\begin{equation}
    L = L_{\text{MSE}} + \lambda L_{\text{cor}},
\end{equation}
where $L_{\text{MSE}}$ is the mean squared error loss, $L_{\text{cor}}$ is the correlation loss, and $\lambda$ is a weighting factor for the correlation loss. The correlation loss is defined as:
\begin{equation}
    L_{\text{cor}} = \frac{1}{N} \sum_{i=1}^{N} (x_i - \bar{x})(y_i - \bar{y}),
\end{equation}
where $x$ and $y$ are the variables of interest, and $\bar{x}$ and $\bar{y}$ are their respective means.

The reason why we don't apply the correlation term between energy and radius is that the relation between them is by experienced, although everyone would expect the result, it's not solid, we don't want to bias our model, or you can say we don't want to tell the answer of the relation to our model. 

However, although the correlation term was added to the loss function and it indeed suppressed the relation between energy and theta, the centralization of the generated samples did not improve significantly. This suggests that the correlation term alone is not sufficient to address the centralization issue.

\begin{figure}[h!]
    \centering
    % Include any relevant results or visualizations here
    \caption{The Picture after adding the correlation term.}
\end{figure}

\section{Conditioning Issue}





